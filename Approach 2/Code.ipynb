{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity Results\n",
      "Unigram Perplexity: 44.10074355133619\n",
      "Bigram Perplexity: 1.739944707396139\n",
      "Trigram Perplexity: 1.0849742603971948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import defaultdict, Counter\n",
    "from nltk import ngrams, FreqDist, ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def load_corpus(directory):\n",
    "    corpus = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "                corpus += file.read() + \" \"\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def preprocess_tokens(tokens):\n",
    "    return [token for token in tokens if not re.search(r'\\d', token)]\n",
    "\n",
    "def train_ngram_models(tokens):\n",
    "    unigram_fd = FreqDist(tokens)\n",
    "    bigram_cfd = ConditionalFreqDist(ngrams(tokens, 2))\n",
    "    trigram_cfd = ConditionalFreqDist([((w1, w2), w3) for w1, w2, w3 in ngrams(tokens, 3)])\n",
    "    return unigram_fd, bigram_cfd, trigram_cfd\n",
    "\n",
    "starting_words = [\"subha\", \"raat\", \"aaj\", \"kal\", \"main\", \"hum\", \"uske\", \"phir\"]\n",
    "\n",
    "def generate_text(model, num_words=10, previous_sentence=None, direction=\"forward\"):\n",
    "    if model == \"unigram\":\n",
    "        return \" \".join(random.choices(list(unigram_fd.keys()), k=num_words))\n",
    "    \n",
    "    elif model == \"bigram\":\n",
    "        sentence = []\n",
    "        \n",
    "        if direction == \"backward\":\n",
    "            current_word = random.choice(list(backward_cfd.keys()))\n",
    "        else:\n",
    "            current_word = previous_sentence[-1] if previous_sentence else random.choice(starting_words)\n",
    "        \n",
    "        sentence.append(current_word)\n",
    "        \n",
    "        for _ in range(num_words - 1):\n",
    "            next_word = None\n",
    "            if current_word in bigram_cfd:\n",
    "                next_word = random.choices(list(bigram_cfd[current_word].keys()), weights=list(bigram_cfd[current_word].values()))[0]\n",
    "            if not next_word:\n",
    "                break\n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "        \n",
    "        return \" \".join(sentence)\n",
    "    \n",
    "    elif model == \"trigram\":\n",
    "        sentence = []\n",
    "        \n",
    "        if previous_sentence and len(previous_sentence) >= 2:\n",
    "            current_context = (previous_sentence[-2], previous_sentence[-1])\n",
    "        else:\n",
    "            current_context = random.choice(list(trigram_cfd.keys()))\n",
    "        \n",
    "        sentence.extend(current_context)\n",
    "        \n",
    "        for _ in range(num_words - 2):\n",
    "            next_word = None\n",
    "            if current_context in trigram_cfd:\n",
    "                next_word = random.choices(list(trigram_cfd[current_context].keys()), weights=list(trigram_cfd[current_context].values()))[0]\n",
    "            if not next_word:\n",
    "                break\n",
    "            sentence.append(next_word)\n",
    "            current_context = (current_context[1], next_word)\n",
    "        \n",
    "        return \" \".join(sentence)\n",
    "    \n",
    "    elif model == \"bidirectional\":\n",
    "        forward_text = generate_text(\"bigram\", num_words//2, direction=\"forward\")\n",
    "        backward_text = generate_text(\"bigram\", num_words//2, direction=\"backward\")\n",
    "        return \" \".join(backward_text.split()[::-1]) + \" \" + forward_text\n",
    "    \n",
    "    return \"Invalid model\"\n",
    "\n",
    "# Backward bigram Model\n",
    "def backward_bigram_model(tokens):\n",
    "    backward_bigrams = list(ngrams(tokens[::-1], 2))\n",
    "    return ConditionalFreqDist(backward_bigrams)\n",
    "\n",
    "#Bidirectional bigram Model\n",
    "def bidirectional_bigram_model(tokens):\n",
    "    forward_cfd = ConditionalFreqDist(list(ngrams(tokens, 2)))\n",
    "    backward_cfd = backward_bigram_model(tokens)\n",
    "    return forward_cfd, backward_cfd\n",
    "\n",
    "# Calculate Perplexity\n",
    "def calculate_perplexity(model, test_tokens, n):\n",
    "    log_probability = 0\n",
    "    N = len(test_tokens)\n",
    "    \n",
    "    if n == 1:\n",
    "        #Unigram model\n",
    "        total_tokens = sum(model.values())\n",
    "        for token in test_tokens:\n",
    "            prob = model[token] / total_tokens if token in model else 0\n",
    "            log_probability += math.log2(prob) if prob > 0 else 0\n",
    "    \n",
    "    elif n == 2:\n",
    "        #Bigram model\n",
    "        for w1, w2 in ngrams(test_tokens, 2):\n",
    "            prob = model[w1][w2] / sum(model[w1].values()) if w1 in model and w2 in model[w1] else 0\n",
    "            log_probability += math.log2(prob) if prob > 0 else 0\n",
    "    \n",
    "    elif n == 3:\n",
    "        #Trigram model\n",
    "        for w1, w2, w3 in ngrams(test_tokens, 3):\n",
    "            prob = model[(w1, w2)][w3] / sum(model[(w1, w2)].values()) if (w1, w2) in model and w3 in model[(w1, w2)] else 0\n",
    "            log_probability += math.log2(prob) if prob > 0 else 0\n",
    "    \n",
    "    perplexity = 2 ** (-log_probability / N)\n",
    "    return perplexity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = load_corpus(\"Diaries\")\n",
    "    tokens = preprocess_tokens(tokenize_text(corpus))\n",
    "    \n",
    "    #80% training, 20% test\n",
    "    split = int(0.8 * len(tokens))\n",
    "    train_tokens = tokens[:split]\n",
    "    test_tokens = tokens[split:]\n",
    "    \n",
    "    unigram_fd, bigram_cfd, trigram_cfd = train_ngram_models(train_tokens)\n",
    "    backward_cfd = backward_bigram_model(train_tokens)\n",
    "    forward_cfd, backward_cfd = bidirectional_bigram_model(train_tokens)\n",
    "    \n",
    "    unigram_perplexity = calculate_perplexity(unigram_fd, test_tokens, 1)\n",
    "    bigram_perplexity = calculate_perplexity(bigram_cfd, test_tokens, 2)\n",
    "    trigram_perplexity = calculate_perplexity(trigram_cfd, test_tokens, 3)\n",
    "    \n",
    "    #unigram\n",
    "    with open(\"unigram_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== Unigram Model ===\\n\")\n",
    "        for i in range(10):\n",
    "            sentence = generate_text('unigram', num_words=random.randint(7, 12))\n",
    "            f.write(f\"Sentence {i+1}: {sentence}\\n\")\n",
    "    \n",
    "    #save bigram\n",
    "    with open(\"bigram_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== Bigram Model ===\\n\")\n",
    "        previous_sentence = None\n",
    "        for i in range(10):\n",
    "            sentence = generate_text(\"bigram\", num_words=random.randint(7, 12), previous_sentence=previous_sentence)\n",
    "            f.write(f\"Sentence {i+1}: {sentence}\\n\")\n",
    "            previous_sentence = sentence.split()\n",
    "    \n",
    "    #save trigram\n",
    "    with open(\"trigram_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== Trigram Model ===\\n\")\n",
    "        previous_sentence = None\n",
    "        for i in range(10):\n",
    "            sentence = generate_text(\"trigram\", num_words=random.randint(7, 12), previous_sentence=previous_sentence)\n",
    "            f.write(f\"Sentence {i+1}: {sentence}\\n\")\n",
    "            previous_sentence = sentence.split()\n",
    "    \n",
    "    #backward bigram\n",
    "    with open(\"backward_bigram_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== Backward Bigram Model ===\\n\")\n",
    "        for i in range(10):\n",
    "            sentence = generate_text('bigram', num_words=random.randint(7, 12), direction='backward')\n",
    "            f.write(f\"Sentence {i+1}: {sentence}\\n\")\n",
    "    \n",
    "    #bidirectional bigram\n",
    "    with open(\"bidirectional_bigram_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== Bidirectional Bigram Model ===\\n\")\n",
    "        for i in range(10):\n",
    "            sentence = generate_text('bidirectional', num_words=random.randint(7, 12))\n",
    "            f.write(f\"Sentence {i+1}: {sentence}\\n\")\n",
    "\n",
    "   \n",
    "    print(\"\\nPerplexity Results\")\n",
    "    print(f\"Unigram Perplexity: {unigram_perplexity}\")\n",
    "    print(f\"Bigram Perplexity: {bigram_perplexity}\")\n",
    "    print(f\"Trigram Perplexity: {trigram_perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
